---
title: "Kristen Analysis"
output: html_document
date: "2025-02-10"
---

```{r}
# Load necessary libraries
library(readxl)   # For reading Excel files
library(dplyr)    # For binding and data manipulation
library(lubridate)  # For handling datetime columns (if needed)
library(tidyverse)  # to manipulate strings and a lot more
library(here)
library(extrafont)
```


# COLLECTION OF CODE CHUNKS FOR CLEANING DATA SPECIFIC TO MAPSS THESIS STUDY
I compiled a number of code chunks that I used while cleaning the original raw data (coded transciptions of participants). It's unlikely I would need these again so I refrained from making eahc into its own r. script. That said, I might use them again for this or a related study and so I wanted them organized all together.

## Setup steps
```{r}
# Set the directory path where your .xlsx files are located
directory_path <- '~/KristenWorkingDirectory/Play_Narrative/PN_Datasets/Pretend_Play/Raw_Data/For_Analysis'

file.exists(directory_path)

# List all .xlsx files in the directory (full paths) and exclude temporary files that start with '~$'
file_list <- list.files(directory_path, pattern = "\\.xls.*$", full.names = TRUE)

# If the file type is a .tsv, comment out the above code and delete the comment hash below
#file_list <- list.files(directory_path, pattern = "\\.tsv$", full.names = TRUE)

# grab names of files to create participant ID column
file_list <- file_list[ !grepl("^~\\$", basename(file_list)) ] 

print(file_list)
```

## Combining function
```{r}
#library(readxl)  # Ensure package is loaded
#library(dplyr)

# load in individual transcripts & combine into one giant file for each data group (P2_H7, P2_H8, P2_H10, P3_H7, P3_H8, P3_H10)
load_and_bind_files <- function(files) {
  if (length(files) == 0) {
    stop("No files found. Check your directory path and file extensions.")
  }

  all_data <- lapply(files, function(file) {
    print(paste("Attempting to read:", file))  

    # The following code detects whether the files are excel files or tsv files and reads them in, or doesn't, depending
    if (grepl("\\.(xlsx|xls)$", file, ignore.case = TRUE)) {
      data <- tryCatch({
        read_excel(file)
      }, error = function(e) {
        print(paste("Error reading Excel file:", file, "-", e$message))
        return(NULL)
      })
    } else if (grepl("\\.tsv$", file, ignore.case = TRUE)) {
      data <- tryCatch({
        read.delim(file, header = TRUE, sep = "\t")
      }, error = function(e) {
        print(paste("Error reading TSV file:", file, "-", e$message))
        return(NULL)
      })
    } else {
      print(paste("Skipping unsupported file:", file))
      return(NULL)
    }
    if (is.null(data)) {
      print(paste("Skipping file due to read error:", file))
      return(NULL)
    }
    print(paste("Rows read from", file, ":", nrow(data)))
    
# The following code reads the filename and creates and populates a column names Participant ID with the ID of each participant based on the filename
    
    # Get the filename (without the path)
    file_name <- basename(file)
    
 # Use a regular expression to extract participant ID and session number
    # The regex pattern looks for one or more digits, a period, then one or more digits
    #   (ex: in "022.10_Final.xlsx":  ([0-9]+) matches "022" ,  \\. matches the period ,  ([0-9]+) matches "10"
    pattern <- "^([0-9]+)\\.([0-9]+)"
    matches <- regexec(pattern, file_name) 
    extracted <- regmatches(file_name, matches)
    if (length(extracted) > 0 && length(extracted[[1]]) >= 3) { # extracted[[1]][1] is the entire match,
      participant_id <- as.numeric(extracted[[1]][2]) # extracted[[1]][2] is the participant ID,
      session <- as.numeric(extracted[[1]][3]) # extracted[[1]][3] is the session number.
    } else {
      # If the filename does not match the expected format, assign NA
      participant_id <- NA
      session <- NA
    }
    
    # Add the new columns to the data frame
    data$participant_id <- participant_id
    data$session <- session
    
    # (Optional) Ensure the 'id' column is character to avoid scientific notation
    if ("id" %in% colnames(data)) {
      data$id <- as.character(data$id)
    }
    
    # (Optional) Convert 'time' column to a consistent datetime format, if it exists
    if ("time" %in% colnames(data)) {
      data$time <- as_datetime(data$time)
    }
    
    # Ensure the 'line' column is always character to prevent type mismatch
    if ("line" %in% colnames(data)) {
        data$line <- as.character(data$line)
    }
    # Ensure 'p_word_num is a character to prevent type mismatch
    if ('p_word_num' %in% colnames(data)) {
        data$p_word_num <- as.character(data$p_word_num)
    }
    # Ensure p_time is a character type to prevent type mismatch
    if ('p_time' %in% colnames(data)) {
        data$p_time <- as.character(data$p_time)
    }
    # Ensure column utt_key is a character type to prevent mismatch
      if ('utt_key' %in% colnames(data)) {
        data$utt_key <- as.character(data$utt_key)
    }
    # Rename MOTHER DXT column to combine with Parent DXT
    if('MOTHER DXT' %in% colnames(data)){
        data <- data %>%
          rename('Parent DXT' = 'MOTHER DXT')
    }
    # Same as above but with 'mother dxt'
    if('mother dxt' %in% colnames(data)){
        data <- data %>%
          rename('Parent DXT' = 'mother dxt')
    }
    # Rename CHILD DXT to Child DXT to combine
    if ('CHILD DXT' %in% colnames(data)){
        data <- data %>%
          rename('Child DXT' = 'CHILD DXT')
    }
    # same as above with 'child dxt'
    if ('child dxt' %in% colnames(data)){
        data <- data %>%
          rename('Child DXT' = 'child dxt')
    }
    # Rename trans_id to trans.ID
    if ('trans_id' %in% colnames(data)){
        data <- data %>%
          rename('trans.ID' = 'trans_id')
    }
    # Rename 'Context' to context'
    if ('Context' %in% colnames(data)){
        data <- data %>%
          rename('context' = 'Context')
    } 
    
    return(data)
  })

  # Remove NULL elements
  all_data <- Filter(Negate(is.null), all_data)

  print(paste("Number of datasets successfully read:", length(all_data)))

  if (length(all_data) == 0) {
    stop("No valid data frames were loaded. Check file formats and paths.")
  }

  combined_data <- bind_rows(all_data)
  return(combined_data)
}
```

## Run the combining function:
```{r}
# Run the function to load, process, and combine the data from all files
combined_dataset <- load_and_bind_files(file_list)

# View the first few rows of the combined dataset
head(combined_dataset)
```

## Create a column that calculates words per utterance for each row with an utterance:
```{r}
# Adding words per utterance column to dataset
combined_dataset <- combined_dataset %>%
  mutate(C_wpu = ifelse(is.na(c_utts) | c_utts == "", 
                                 0,  # Assign 0 if the cell is empty or NA
                                 str_count(c_utts, "\\S+")  # Count words using regex
                                 ))
head(combined_dataset)
```

## Save combined_data as CSV to specified location:
```{r}
write.csv(
  combined_dataset,
  here("PN_Datasets", "Pretend_Play", "CSVs_of_Combined_Data_PP", "P3_H10_cwpu.csv"),
  row.names = FALSE
)
```

## Get rid of extra columns:
```{r}
P3_H10_ALL <- P3_H10_cwpu %>%
  select("trans.ID", "time", "line", "key", "p_utts", "p_form", "p_lrb", "p_obj", "p_gloss", "p_orient", "p_mspd", "c_utts", "c_form", "c_lrb", "c_obj", "c_gloss", "c_orient", "c_mspd", "context", "c_pret", "participant_id", "session", "C_wpu",  "comments_KJ")
```

## Save necessary data as CSV:
```{r}
write.csv(
  P3_H10_ALL,
  here("PN_Datasets", "Pretend_Play", "CSVs_of_Combined_Data_PP", "P3_H10_ALL.csv"),
  row.names = FALSE
)
```

## Get descriptive statistics for ALL Gestures:
```{r}
# Load necessary libraries
library(dplyr)   # For data manipulation
library(stringr) # For string functions

# Count the number of times individual participants made any gesture
gesture_count <- combined_dataset %>%
  group_by(participant_id) %>% # Groups data by participant ID
  summarise(count_non_na = sum(!is.na(c_form))) # Counts number of non NA values

print(gesture_count)
```

## Get total number of gestures among all observations of all participants:
```{r}
# Sum of all instances of gesture as counted in the non NA values in the above data frame
all_gestures <- sum(gesture_count$count_non_na)

print(all_gestures)
```

Get the Mean and Standard deviation of ALL gestures :
```{r, echo=FALSE}
# get summary statistics of the count_non_na column of the above dataset
gesture_summary <- gesture_count %>% 
  summarise(
    mean_gesture = mean(count_non_na),
    sd_gesture = sd(count_non_na)
  )

print(gesture_summary)
```

# Get descriptive for iconic gestures: 
## If inspecting a df that only contains the instances of iconic gestures:
```{r}
# Load necessary libraries
#library(dplyr)   # For data manipulation
#library(stringr) # For string functions

# Create a new data frame that includes rows where 'iconic' appears anywhere in c_form
iconic_df <- combined_dataset %>%
  filter(str_detect(str_to_lower(c_form), "iconic"))

# View the first few rows of the new data frame
head(iconic_df)
```

## If inspecting how often individual kids do iconic gestures: 
```{r}
# Count occurrences of 'iconic' for each participant_id
iconic_counts <- iconic_df %>%
  group_by(participant_id) %>%
  summarise(iconic_count = sum(str_detect(str_to_lower(c_form), "iconic"), na.rm = TRUE), .groups = "drop")

# Print results
print(iconic_counts)
```

## Total number of iconic gestures across all participants:
```{r}
# The sum of all instances of iconic gestures in the above column
all_iconic <- sum(iconic_counts$iconic_count)

print(all_iconic)
```

## Summary statistics across participants: 
```{r}
# Calculate mean and standard deviation of 'iconic' counts across participants
iconic_summary <- iconic_counts %>%
  summarise(
    mean_iconic = mean(iconic_count, na.rm = TRUE),
    sd_iconic = sd(iconic_count, na.rm = TRUE)
  )

# Print results
print(iconic_summary)
```

## Calculate instances of Pretend Play:
```{r}
# Count the number of times individual participants engaged in pretend play
pp_count <- combined_dataset %>%
  group_by(participant_id) %>%  # Groups data by participant ID
  summarise(
    #count_p_combined = sum(C_Pret, na.rm = TRUE) # for the C_Pret column name
    count_p_combined = sum(str_detect(str_to_lower(`Child DXT`), "p"), na.rm = TRUE) 
    # In at least one dataset there are two Child DXT columns thanks to one file not capitalizing it. if this is the case, comment out the above line and remove the comment hash in front of the code below
    #count_p_combined = sum(str_detect(str_to_lower(`Child DXT`), "p"), na.rm = TRUE) + sum(str_detect(str_to_lower(`CHILD DXT`), "p"), na.rm = TRUE)  
  )

print(pp_count)
```

## Total number of instances of pretend play:
```{r}
# Sum the coun_p_combined column from above
all_pp <- sum(pp_count$count_p_combined)

print(all_pp)
```

## Descriptive statistics of all instances of pretend play:
```{r}
# calculate the summary statistics of count_p_combined
pp_summary <- pp_count %>%
  summarise(
    mean_pp = mean(count_p_combined, na.rm = TRUE),
    sd_pp = sd(count_p_combined, na.rm = TRUE)
  )

# Print results
print(pp_summary)
```

## Mean of words per utterance for each child
```{r}
# finding the average words per utterance per participant
avg_words_per_utt <- combined_dataset %>%
  group_by(participant_id) %>%
  summarise(
    total_words = sum(C_wpu[C_wpu > 0], na.rm = TRUE),  # Sum of non-zero word counts
    total_utts = sum(C_wpu > 0, na.rm = TRUE),  # Count of non-zero utterances
    avg_c_wpu = ifelse(total_utts > 0, total_words / total_utts, NA)  # Calculate mean, avoid divide by zero
  )

print(avg_words_per_utt)
```

## SD of words per utterance for each child
```{r}
# finding the standard deviation for words per utterance per participant
avg_words_per_utt <- combined_dataset%>%
  group_by(participant_id) %>%
  summarise(
    total_words = sum(C_wpu[C_wpu > 0], na.rm = TRUE),  # Sum of non-zero word counts
    total_utts = sum(C_wpu > 0, na.rm = TRUE),  # Count of non-zero utterances
    avg_c_wpu = ifelse(total_utts > 0, total_words / total_utts, NA),  # Mean words per utterance
    sd_c_wpu = ifelse(total_utts > 1, sd(C_wpu[C_wpu > 0], na.rm = TRUE), NA)  # Standard deviation
  )

print(avg_words_per_utt)
```

## Descriptive statistics of words per utterance
```{r}
# calculate the summary statistics of count_p_combined
c_wpu_summary <- avg_words_per_utt %>%
  summarise(
    c_total_words = sum(total_words, na.rm = TRUE),
    mean_c_wpu = mean(avg_c_wpu, na.rm = TRUE),
    sd_c_wpu = sd(avg_c_wpu, na.rm = TRUE)
  )

# Print results
print(c_wpu_summary)
```
